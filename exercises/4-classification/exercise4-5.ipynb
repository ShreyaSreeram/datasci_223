{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUBk6fPc8FyR"
      },
      "source": [
        "# Classification on `emnist`\n",
        "\n",
        "## 1. Create `Readme.md` to document your work\n",
        "\n",
        "Explain your choices, process, and outcomes.\n",
        "\n",
        "## 2. Classify ~~all symbols~~ letters a -> g\n",
        "\n",
        "### Subset the data\n",
        "\n",
        "Select only the lowercase letters (a, b, ..., g) for classification\n",
        "\n",
        "### Choose a model\n",
        "\n",
        "Your choice of model! Choose wisely...\n",
        "\n",
        "### Train away!\n",
        "\n",
        "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
        "\n",
        "### Evaluate the model\n",
        "\n",
        "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
        "\n",
        "### Investigate subsets\n",
        "\n",
        "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
        "\n",
        "### Improve performance\n",
        "\n",
        "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
        "\n",
        "## 3. Model showdown: upper vs lowercase on abcXYZ\n",
        "\n",
        "### Subset the data\n",
        "\n",
        "Select out the set of upper- and lowercase (a, b, c, x, y z, A, B, C, X, Y, Z). Note that some of these classes can be confusing (e.g., x and y).\n",
        "\n",
        "### Train and tune models\n",
        "\n",
        "Perform a full model training and hyperparameter tuning.\n",
        "\n",
        "1. Select candidate models, hyperparameter options, and evaluation metric\n",
        "2. Set aside a validation hold-out dataset\n",
        "3. Train models over K splits (use k-fold or train/test split)\n",
        "    1. Split train using k-fold with the number of folds equal to the number of parameter combinations\n",
        "    2. Train on k-fold split\n",
        "    3. Record performance of each set of parameters\n",
        "    4. Use winning set of parameters to train model on full training set\n",
        "    5. Record each model's performance on that split's test set\n",
        "4. Evaluate model performance and promote one model as the winner\n",
        "5. Train winning model on both train + test\n",
        "6. Check model performance on the validation hold-out\n",
        "\n",
        "\n",
        "## 4. (_Optional_) Model comparison: classify even vs odd\n",
        "\n",
        "**NOTE:** This is a larger dataset (~400k rows) so it will require more memory and time to train models on it.\n",
        "\n",
        "Alternatively, you can train models on smaller subsets of the data to get a feel for which models perform better than others. Then train the winning model on the full dataset and validate against the hold-out.\n",
        "\n",
        "### Subset the data\n",
        "\n",
        "Select only digits and add a column for 'is_even'. Be sure to create a validation hold-out dataset for later.\n",
        "\n",
        "### Build and compare models\n",
        "\n",
        "Train at least two different models, compare the results and choose a winner based on an evaluation metric of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KgZVlXAd8FyU"
      },
      "outputs": [],
      "source": [
        "%pip install -q emnist pandas pyarrow numpy matplotlib seaborn scikit-learn xgboost tensorflow\n",
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GpzqFabY8FyU"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import emnist\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# ML packages\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import ParameterGrid, KFold, cross_val_score\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# XGBoost (SVM)\n",
        "from xgboost import XGBClassifier\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Constants\n",
        "SIZE = 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sTMQVG28FyU"
      },
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "def int_to_char(label):\n",
        "    \"\"\"Convert an integer label to the corresponding uppercase character.\"\"\"\n",
        "    if label < 10:\n",
        "        return str(label)\n",
        "    elif label < 36:\n",
        "        return chr(label - 10 + ord('A'))\n",
        "    else:\n",
        "        return chr(label - 36 + ord('a'))\n",
        "\n",
        "def show_image(row):\n",
        "    \"\"\"Display a single image and its corresponding label.\"\"\"\n",
        "    image = row['image']\n",
        "    label = row['label']\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title('Label: ' + int_to_char(label))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_grid(data, title=None, num_cols=5, figsize=(20, 10)):\n",
        "    \"\"\"\n",
        "    Display a list of images as a grid of num_cols columns.\n",
        "    images: a list of images, each represented as a 28x28 numpy array\n",
        "    labels: a list of labels, one for each image\n",
        "    title: (optional) a title for the plot\n",
        "    num_cols: (optional) number of columns to use in the grid\n",
        "    figsize: (optional) size of the figure\n",
        "    \"\"\"\n",
        "    num_images = len(data)\n",
        "    num_rows = (num_images - 1) // num_cols + 1\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    if title is not None:\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            index = i * num_cols + j\n",
        "            if index < num_images:\n",
        "                axes[i, j].imshow(data.iloc[index]['image'], cmap='gray')\n",
        "                axes[i, j].axis('off')\n",
        "                label = int_to_char(data.iloc[index]['label'])\n",
        "                axes[i, j].set_title(label)\n",
        "    plt.show()\n",
        "\n",
        "# Get a random image of a given label from the dataset\n",
        "def get_image_by_label(data, label):\n",
        "    \"\"\"Get a random image of a given label from the dataset.\"\"\"\n",
        "    images = data[data['label'] == label]['image'].tolist()\n",
        "    return random.choice(images)\n",
        "\n",
        "# Plot the training and validation accuracy during the training of a model\n",
        "def plot_accuracy(history):\n",
        "    \"\"\"Plot the training and validation accuracy during the training of a model.\"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training and validation loss during the training of a model\n",
        "def plot_loss(history):\n",
        "    \"\"\"Plot the training and validation loss during the training of a model.\"\"\"\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Normalize the pixel values of the images in the dataset to have zero mean and unit variance\n",
        "# This is a common preprocessing step for neural networks, but may not be necessary in all cases\n",
        "def normalize_images(images):\n",
        "    \"\"\"Normalize the pixel values of the images in the dataset to have zero mean and unit variance.\"\"\"\n",
        "    images = np.array(images)\n",
        "    mean = images.mean()\n",
        "    std = images.std()\n",
        "    images = (images - mean) / std\n",
        "    return images.tolist()\n",
        "\n",
        "# Display metrics for a model\n",
        "def display_metrics(task, model_name, metrics_dict):\n",
        "    \"\"\"Display performance metrics and confusion matrix for a model.\"\"\"\n",
        "    metrics_df = pd.DataFrame()\n",
        "    cm_df = pd.DataFrame()\n",
        "    for key, value in metrics_dict[task][model_name].items():\n",
        "        if type(value) == np.ndarray:\n",
        "            cm_df = pd.DataFrame(value, index=['actual 0', 'actual 1'], columns=['predicted 0', 'predicted 1'])\n",
        "        else:\n",
        "            metrics_df[key] = [value]\n",
        "    display(Markdown(f'# Performance Metrics: {model_name}'))\n",
        "    display(metrics_df)\n",
        "    display(Markdown(f'# Confusion Matrix: {model_name}'))\n",
        "    display(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6r7B6th8FyV",
        "outputId": "df08624d-d145-4af8-edf6-4fab451df1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist.zip: 536MB [00:05, 99.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "\n",
        "# Extract the training split as images and labels\n",
        "image, label = emnist.extract_training_samples('byclass')\n",
        "\n",
        "# Add columns for each pixel value (28x28 = 784 columns)\n",
        "emnist_train = pd.DataFrame()\n",
        "\n",
        "# Add a column with the image data as a 28x28 array\n",
        "emnist_train['image'] = list(image)\n",
        "emnist_train['image_flat'] = emnist_train['image'].apply(lambda x: np.array(x).reshape(-1))\n",
        "\n",
        "# Add a column showing the label\n",
        "emnist_train['label'] = label\n",
        "\n",
        "# Convert labels to characters\n",
        "class_label = np.array([int_to_char(l) for l in label])\n",
        "\n",
        "# Add a column with the character corresponding to the label\n",
        "emnist_train['class'] = class_label\n",
        "\n",
        "# Repeat for the test split\n",
        "image, label = emnist.extract_test_samples('byclass')\n",
        "class_label = np.array([int_to_char(l) for l in label])\n",
        "emnist_test = pd.DataFrame()\n",
        "emnist_test['image'] = list(image)\n",
        "emnist_test['image_flat'] = emnist_test['image'].apply(lambda x: np.array(x).reshape(-1))\n",
        "emnist_test['label'] = label\n",
        "emnist_test['class'] = class_label\n",
        "\n",
        "# Combine the training and test splits\n",
        "emnist_all = pd.concat([emnist_train, emnist_test], axis=0)\n",
        "\n",
        "# Subset for only digits 0-9\n",
        "digits = emnist_all[emnist_all['label'] < 10]\n",
        "\n",
        "# Subset for lowercase letters\n",
        "lowercase = emnist_all[(emnist_all['class'] >= 'a') & (emnist_all['class'] <= 'z')]\n",
        "uppercase = emnist_all[(emnist_all['class'] >= 'A') & (emnist_all['class'] <= 'Z')]\n",
        "\n",
        "# Subset for upper- and lowercase letters a, b, c, d, e, f, g\n",
        "a2g = emnist_all[(emnist_all['class'].isin(['a', 'b', 'c', 'd', 'e', 'f', 'g']))]\n",
        "\n",
        "# Subset for upper- and lowercase letters a, b, c, x, y, z\n",
        "abcxyz = emnist_all[(emnist_all['class'].isin(['a', 'b', 'c', 'A', 'B', 'C', \\\n",
        "                                               'x', 'y', 'z', 'X', 'Y', 'Z']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "0sJAhZx38FyV",
        "outputId": "5336509a-2038-4ee5-aa22-3edfcc08a02f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Dataset Sizes"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**a2g**: 68795"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**abcxyz**: 65926"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**digits**: 402953"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**emnist_all**: 814255"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display the size of a2g, abcxyz, digits, and the full dataset\n",
        "display(Markdown(f'# Dataset Sizes'))\n",
        "display(Markdown(f'**a2g**: {len(a2g)}'))\n",
        "display(Markdown(f'**abcxyz**: {len(abcxyz)}'))\n",
        "display(Markdown(f'**digits**: {len(digits)}'))\n",
        "display(Markdown(f'**emnist_all**: {len(emnist_all)}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js-pKCmx8FyV"
      },
      "outputs": [],
      "source": [
        "# FIXME: Classify lettters as uppercase/lowercase\n",
        "abcxyz['is_upper'] = ...\n",
        "\n",
        "# FIXME: Classify digits as even/odd\n",
        "digits['is_even'] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "hm704O968FyV",
        "outputId": "64d04612-50e2-4887-f878-58edb11f1ab8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-57418aef5db1>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mX_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fold_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0my_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fold_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# 3.B. Train on k-fold split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "#Model 1: Random Forest\n",
        "\n",
        "#Import ML packages for random forest:\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Select candidate models, hyperparameter options, and evaluation metric\n",
        "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "rf_param_grid = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
        "scoring_metric = 'accuracy'\n",
        "\n",
        "\n",
        "#Training models over k-splits:\n",
        "models = [\n",
        "   (RandomForestClassifier(), rf_param_grid),\n",
        "]\n",
        "\n",
        "\n",
        "#Validation hold-out set:\n",
        "X = a2g['image_flat'].tolist()\n",
        "\n",
        "\n",
        "Y= a2g['class'].tolist()\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#Training models over k-splits:\n",
        "\n",
        "models = [\n",
        "    (RandomForestClassifier(), rf_param_grid),\n",
        "]\n",
        "\n",
        "for model, param_grid in models:\n",
        "    for param_combination in ParameterGrid(param_grid):\n",
        "        model.set_params(**param_combination)\n",
        "        fold_scores = []\n",
        "\n",
        "        # 3.A. Split train using k-fold with the number of folds\n",
        "\t\t\t\t\t\t  # equal to the number of parameter combinations\n",
        "        for train_index, test_index in kf.split(X_train):\n",
        "            X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "            y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "            # 3.B. Train on k-fold split\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "            # 3.C. Record performance of each set of parameters\n",
        "            y_pred = model.predict(X_fold_test)\n",
        "            fold_score = accuracy_score(y_fold_test, y_pred)\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "        # 3.D. Use winning set of parameters to train model on full training set\n",
        "        avg_score = np.mean(fold_scores)\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_model = model.get_params()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Select candidate models, hyperparameter options, and evaluation metric\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "rf_param_grid = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
        "scoring_metric = 'accuracy'\n",
        "\n",
        "\n",
        "#Training models over k-splits:\n",
        "models = [\n",
        "   (RandomForestClassifier(), rf_param_grid),\n",
        "]\n",
        "\n",
        "\n",
        "#Validation hold-out set:\n",
        "X = a2g['image_flat'].tolist()\n",
        "\n",
        "\n",
        "Y= a2g['class'].tolist()\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "EeIa5VAP-OIz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross-validation score:\n",
        "best_score = 0\n",
        "best_model = None\n",
        "for model, param_grid in models:\n",
        "    for param_combination in ParameterGrid(param_grid):\n",
        "        model.set_params(**param_combination)\n",
        " # cross_val_score returns array of scores, which you need to take the avg\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "        avg_score = np.mean(scores)\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_model = (model)\n"
      ],
      "metadata": {
        "id": "LERkJmpBHvlA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Model Parameters: {best_model}\")\n",
        "print(f\"Best Model Average Cross-Validation Score: {best_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER_AzzKrMg93",
        "outputId": "00d5422d-103a-41f2-f021-bed26b12ea68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Parameters: RandomForestClassifier(max_depth=20)\n",
            "Best Model Average Cross-Validation Score: 0.959917103492281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the winning model on both train+test\n",
        "\n",
        "# Access the best classifier (best_model[0]) + hyperparameters (**best_model[1])# Remember X_train is the train+test for when we did K-Fold CV.\n",
        "\n",
        "\n",
        "best_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "RbBo-Fdf3SIJ",
        "outputId": "0b6a07b8-a9ba-4395-ce0e-b1d6b7af438b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=20)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=20)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance metrics of the model:\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "validation_score = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Hold-out Score: {validation_score}\")\n",
        "prec = precision_score(y_val, y_val_pred, average='macro', zero_division = 1)\n",
        "print(f\"Precision: {prec}\")\n",
        "rec = recall_score(y_val, y_val_pred, average = 'macro')\n",
        "print(f\"Recall: {rec}\")\n",
        "f1 = f1_score(y_val, y_val_pred, average = 'macro')\n",
        "print(f\"f1: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "507-9lNw5ecD",
        "outputId": "71ecec47-20f3-49be-e666-14b35b5df037"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Hold-out Score: 0.9615524384039538\n",
            "Precision: 0.9586551694445034\n",
            "Recall: 0.9281768348308493\n",
            "f1: 0.942181871846295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "cm = confusion_matrix(y_val, y_val_pred, labels=best_model.classes_)\n",
        "print(\"Confusion Matrix:\")\n",
        "cm_df = pd.DataFrame(cm, index=best_model.classes_, columns=best_model.classes_)\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywlNiWUhIJW",
        "outputId": "abb2f57c-69c7-4bc4-f349-98e9ec095112"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      a     b    c     d     e    f    g\n",
            "a  2367     4    2    11    46    2   15\n",
            "b     6  1149    1    19     6    7    6\n",
            "c    18     0  526     2    86    1    1\n",
            "d    32    17    0  2267     4    6    2\n",
            "e    26     3   19     6  5654    7    1\n",
            "f     7     3    1    11    13  552    6\n",
            "g    82     3    1    16    22    8  715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3:Model showdown: upper vs lowercase on abcXYZ\n",
        "\n",
        "# Data splitting\n",
        "X_2=abcxyz['image_flat'].tolist()\n",
        "y_2=abcxyz['class']. tolist()\n",
        "\n",
        "xgb_param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "rf_param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20]}\n",
        "lr_param_grid = {\n",
        "    'C': [0.1, 1.0, 10.0],             # Regularization parameter\n",
        "    'solver': ['liblinear', 'lbfgs'],  # Optimization algorithm\n",
        "    'max_iter': [500, 600, 700]         # Maximum number of iterations\n",
        "}\n",
        "\n",
        "\n",
        "# If it's not liking the given labels in y, try re-encoding them:\n",
        "# label_encoder = LabelEncoder()\n",
        "# y = label_encoder. fit_transform(y)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "# Define how many folds you want to split on\n",
        "kf1 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# What models?\n",
        "models_2 = [(RandomForestClassifier(), rf_param_grid),\n",
        "    (XGBClassifier(), xgb_param_grid),\n",
        "    (LogisticRegression(), lr_param_grid)\n",
        "]\n",
        "\n",
        "# Initialize best score and best model\n",
        "top_score = 0\n",
        "top_model = None\n",
        "\n",
        "for models_2, param_grid in models_2:\n",
        "    for param_combination in ParameterGrid(param_grid):\n",
        "        models_2.set_params(**param_combination)\n",
        "\n",
        "# cross_val_score returns array of scores, which you need to take the avg\n",
        "scores = cross_val_score(models_2, X_train_2, y_train_2, cv=kf1, scoring='accuracy')\n",
        "avg_score = np. mean (scores)\n",
        "if avg_score > top_score:\n",
        "  top_score = avg_score\n",
        "  top_model = (models_2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbVXQfkP1F4J",
        "outputId": "52907a12-1c8f-4ea9-a653-36c07675d12b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Model Parameters: {top_model}\")\n",
        "print(f\"Best Model Average Cross-Validation Score: {top_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FTfYLtmcUL3",
        "outputId": "c8c60c97-1852-4b25-e0df-9ef22d3644db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Parameters: LogisticRegression(C=10.0, max_iter=700)\n",
            "Best Model Average Cross-Validation Score: 0.7663064087978764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the winning model on both train+test\n",
        "\n",
        "top_model.fit(X_train_2, y_train_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "XGZUqFxGpGTd",
        "outputId": "41caa4a6-f6b9-4dd4-9db9-e78ebab3a6e6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10.0, max_iter=700)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10.0, max_iter=700)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10.0, max_iter=700)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking model performance on the validation set\n",
        "top_model.fit(X_train_2,y_train_2)\n",
        "y_val_pred2 = top_model.predict(X_val_2)\n",
        "validation_score = accuracy_score(y_val_2, y_val_pred2)\n",
        "print(f\"Validation Hold-out Score: {validation_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYVDGEixpUmJ",
        "outputId": "b17aed55-a509-4791-8237-42277acacbc9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Hold-out Score: 0.772106779918095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion matrix\n",
        "cm_2= confusion_matrix(y_val_2, y_val_pred2, labels = top_model.classes_)\n",
        "print(\"Confusion Matrix:\")\n",
        "cm_df_2 = pd.DataFrame(cm_2, index=top_model.classes_, columns=top_model.classes_)\n",
        "print(cm_df_2)"
      ],
      "metadata": {
        "id": "oH8tSk4BYgYY",
        "outputId": "2701ad51-b66a-4dc7-de1c-612d479c3663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [13186, 13759]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-49a5ace42a8a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm_2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcm_df_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_df_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13186, 13759]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}